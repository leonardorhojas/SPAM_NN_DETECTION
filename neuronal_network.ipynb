{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.read_csv('final_dataset.csv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spam \n",
    "spam_df = data_frame[data_frame.spam == 1]\n",
    "# no spam\n",
    "no_spam_df = data_frame[data_frame.spam == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training data\n",
    "test_spam, train_spam = train_test_split(spam_df, test_size=3000, shuffle=True)\n",
    "test_no_spam, train_no_spam = train_test_split(no_spam_df, test_size=7000, shuffle=True)\n",
    "\n",
    "# merge datasets\n",
    "train = pd.concat([train_spam, train_no_spam])\n",
    "test = pd.concat([test_spam, test_no_spam])\n",
    "\n",
    "# x and y\n",
    "X_train = train[['g1','g2','g3','g4','g5','g6','g7']]\n",
    "Y_train = train[['spam']]\n",
    "X_test = test[['g1','g2','g3','g4','g5','g6','g7']]\n",
    "Y_test = test[['spam']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:916: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.61119453\n",
      "Iteration 2, loss = 0.61039231\n",
      "Iteration 3, loss = 0.61040307\n",
      "Iteration 4, loss = 0.60802783\n",
      "Iteration 5, loss = 0.60606759\n",
      "Iteration 6, loss = 0.60266705\n",
      "Iteration 7, loss = 0.59585014\n",
      "Iteration 8, loss = 0.58620439\n",
      "Iteration 9, loss = 0.57172990\n",
      "Iteration 10, loss = 0.55245667\n",
      "Iteration 11, loss = 0.52894690\n",
      "Iteration 12, loss = 0.50178492\n",
      "Iteration 13, loss = 0.47574732\n",
      "Iteration 14, loss = 0.45076940\n",
      "Iteration 15, loss = 0.42897115\n",
      "Iteration 16, loss = 0.40963845\n",
      "Iteration 17, loss = 0.39503821\n",
      "Iteration 18, loss = 0.38202848\n",
      "Iteration 19, loss = 0.37202215\n",
      "Iteration 20, loss = 0.36233342\n",
      "Iteration 21, loss = 0.35740602\n",
      "Iteration 22, loss = 0.34977837\n",
      "Iteration 23, loss = 0.34586095\n",
      "Iteration 24, loss = 0.34316759\n",
      "Iteration 25, loss = 0.33857648\n",
      "Iteration 26, loss = 0.33565033\n",
      "Iteration 27, loss = 0.33306533\n",
      "Iteration 28, loss = 0.32994442\n",
      "Iteration 29, loss = 0.32966010\n",
      "Iteration 30, loss = 0.32753290\n",
      "Iteration 31, loss = 0.32561002\n",
      "Iteration 32, loss = 0.32397545\n",
      "Iteration 33, loss = 0.32470580\n",
      "Iteration 34, loss = 0.32486241\n",
      "Iteration 35, loss = 0.32249537\n",
      "Iteration 36, loss = 0.31999962\n",
      "Iteration 37, loss = 0.31971215\n",
      "Iteration 38, loss = 0.31903788\n",
      "Iteration 39, loss = 0.31966756\n",
      "Iteration 40, loss = 0.31721151\n",
      "Iteration 41, loss = 0.31695006\n",
      "Iteration 42, loss = 0.31661766\n",
      "Iteration 43, loss = 0.31576289\n",
      "Iteration 44, loss = 0.31770389\n",
      "Iteration 45, loss = 0.31320227\n",
      "Iteration 46, loss = 0.31275213\n",
      "Iteration 47, loss = 0.31088051\n",
      "Iteration 48, loss = 0.30936068\n",
      "Iteration 49, loss = 0.30856521\n",
      "Iteration 50, loss = 0.30854560\n",
      "Iteration 51, loss = 0.30796685\n",
      "Iteration 52, loss = 0.30757355\n",
      "Iteration 53, loss = 0.30604387\n",
      "Iteration 54, loss = 0.30545061\n",
      "Iteration 55, loss = 0.30691821\n",
      "Iteration 56, loss = 0.30496497\n",
      "Iteration 57, loss = 0.30518681\n",
      "Iteration 58, loss = 0.30421035\n",
      "Iteration 59, loss = 0.30509390\n",
      "Iteration 60, loss = 0.30431130\n",
      "Iteration 61, loss = 0.30514854\n",
      "Iteration 62, loss = 0.30307778\n",
      "Iteration 63, loss = 0.30382198\n",
      "Iteration 64, loss = 0.30285312\n",
      "Iteration 65, loss = 0.30258861\n",
      "Iteration 66, loss = 0.30062030\n",
      "Iteration 67, loss = 0.30332401\n",
      "Iteration 68, loss = 0.30184173\n",
      "Iteration 69, loss = 0.30120035\n",
      "Iteration 70, loss = 0.29924286\n",
      "Iteration 71, loss = 0.29945106\n",
      "Iteration 72, loss = 0.29946198\n",
      "Iteration 73, loss = 0.29875332\n",
      "Iteration 74, loss = 0.29964931\n",
      "Iteration 75, loss = 0.29739396\n",
      "Iteration 76, loss = 0.29914956\n",
      "Iteration 77, loss = 0.29745398\n",
      "Iteration 78, loss = 0.29805456\n",
      "Iteration 79, loss = 0.30105415\n",
      "Iteration 80, loss = 0.29755514\n",
      "Iteration 81, loss = 0.29718587\n",
      "Iteration 82, loss = 0.29647879\n",
      "Iteration 83, loss = 0.29582885\n",
      "Iteration 84, loss = 0.29622333\n",
      "Iteration 85, loss = 0.29710834\n",
      "Iteration 86, loss = 0.29658011\n",
      "Iteration 87, loss = 0.29400894\n",
      "Iteration 88, loss = 0.29442037\n",
      "Iteration 89, loss = 0.29769985\n",
      "Iteration 90, loss = 0.29478418\n",
      "Iteration 91, loss = 0.29392896\n",
      "Iteration 92, loss = 0.29664525\n",
      "Iteration 93, loss = 0.29521713\n",
      "Iteration 94, loss = 0.29920172\n",
      "Iteration 95, loss = 0.29576713\n",
      "Iteration 96, loss = 0.30094521\n",
      "Iteration 97, loss = 0.29468602\n",
      "Iteration 98, loss = 0.29297700\n",
      "Iteration 99, loss = 0.29229781\n",
      "Iteration 100, loss = 0.29487831\n",
      "Iteration 101, loss = 0.29283238\n",
      "Iteration 102, loss = 0.29213217\n",
      "Iteration 103, loss = 0.29241772\n",
      "Iteration 104, loss = 0.29062613\n",
      "Iteration 105, loss = 0.29095969\n",
      "Iteration 106, loss = 0.29204247\n",
      "Iteration 107, loss = 0.29151352\n",
      "Iteration 108, loss = 0.29060597\n",
      "Iteration 109, loss = 0.29079089\n",
      "Iteration 110, loss = 0.29191680\n",
      "Iteration 111, loss = 0.29116791\n",
      "Iteration 112, loss = 0.29097978\n",
      "Iteration 113, loss = 0.29257154\n",
      "Iteration 114, loss = 0.29198089\n",
      "Iteration 115, loss = 0.29094832\n",
      "Iteration 116, loss = 0.28980326\n",
      "Iteration 117, loss = 0.28954896\n",
      "Iteration 118, loss = 0.29101904\n",
      "Iteration 119, loss = 0.29097145\n",
      "Iteration 120, loss = 0.29226463\n",
      "Iteration 121, loss = 0.28893833\n",
      "Iteration 122, loss = 0.28946262\n",
      "Iteration 123, loss = 0.29081478\n",
      "Iteration 124, loss = 0.28946935\n",
      "Iteration 125, loss = 0.28920098\n",
      "Iteration 126, loss = 0.29331229\n",
      "Iteration 127, loss = 0.28904965\n",
      "Iteration 128, loss = 0.28962210\n",
      "Iteration 129, loss = 0.29023503\n",
      "Iteration 130, loss = 0.28956474\n",
      "Iteration 131, loss = 0.28894216\n",
      "Iteration 132, loss = 0.28884626\n",
      "Iteration 133, loss = 0.28887872\n",
      "Iteration 134, loss = 0.28955851\n",
      "Iteration 135, loss = 0.28721612\n",
      "Iteration 136, loss = 0.28930295\n",
      "Iteration 137, loss = 0.28891984\n",
      "Iteration 138, loss = 0.28721266\n",
      "Iteration 139, loss = 0.28690832\n",
      "Iteration 140, loss = 0.28973466\n",
      "Iteration 141, loss = 0.28740333\n",
      "Iteration 142, loss = 0.28726911\n",
      "Iteration 143, loss = 0.28995768\n",
      "Iteration 144, loss = 0.28766564\n",
      "Iteration 145, loss = 0.28877827\n",
      "Iteration 146, loss = 0.28718685\n",
      "Iteration 147, loss = 0.28693647\n",
      "Iteration 148, loss = 0.28748932\n",
      "Iteration 149, loss = 0.28872772\n",
      "Iteration 150, loss = 0.28823981\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Results case  0 :\n",
      "Accurancy:  0.7720646400723246\n",
      "TN: 0.0648660865634535\n",
      "FP: 0.005198327494632162\n",
      "FN: 0.22273703243304327\n",
      "TP: 0.7071985535088711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:916: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.63488159\n",
      "Iteration 2, loss = 0.61011217\n",
      "Iteration 3, loss = 0.60947553\n",
      "Iteration 4, loss = 0.60871504\n",
      "Iteration 5, loss = 0.60752306\n",
      "Iteration 6, loss = 0.60556531\n",
      "Iteration 7, loss = 0.60336267\n",
      "Iteration 8, loss = 0.59990130\n",
      "Iteration 9, loss = 0.59448412\n",
      "Iteration 10, loss = 0.58654563\n",
      "Iteration 11, loss = 0.57591267\n",
      "Iteration 12, loss = 0.56070969\n",
      "Iteration 13, loss = 0.54434292\n",
      "Iteration 14, loss = 0.52129188\n",
      "Iteration 15, loss = 0.49698652\n",
      "Iteration 16, loss = 0.47326019\n",
      "Iteration 17, loss = 0.45059196\n",
      "Iteration 18, loss = 0.42992860\n",
      "Iteration 19, loss = 0.41130251\n",
      "Iteration 20, loss = 0.39575385\n",
      "Iteration 21, loss = 0.38266173\n",
      "Iteration 22, loss = 0.37113206\n",
      "Iteration 23, loss = 0.36336670\n",
      "Iteration 24, loss = 0.35611197\n",
      "Iteration 25, loss = 0.35183254\n",
      "Iteration 26, loss = 0.34664423\n",
      "Iteration 27, loss = 0.34020620\n",
      "Iteration 28, loss = 0.33731111\n",
      "Iteration 29, loss = 0.33351711\n",
      "Iteration 30, loss = 0.33127586\n",
      "Iteration 31, loss = 0.32997902\n",
      "Iteration 32, loss = 0.32906625\n",
      "Iteration 33, loss = 0.32631883\n",
      "Iteration 34, loss = 0.32387211\n",
      "Iteration 35, loss = 0.32413405\n",
      "Iteration 36, loss = 0.32366890\n",
      "Iteration 37, loss = 0.32101239\n",
      "Iteration 38, loss = 0.31747811\n",
      "Iteration 39, loss = 0.31743014\n",
      "Iteration 40, loss = 0.31704656\n",
      "Iteration 41, loss = 0.31573411\n",
      "Iteration 42, loss = 0.31498974\n",
      "Iteration 43, loss = 0.31613607\n",
      "Iteration 44, loss = 0.31273635\n",
      "Iteration 45, loss = 0.31309495\n",
      "Iteration 46, loss = 0.31133898\n",
      "Iteration 47, loss = 0.31125191\n",
      "Iteration 48, loss = 0.30992100\n",
      "Iteration 49, loss = 0.30906956\n",
      "Iteration 50, loss = 0.30858489\n",
      "Iteration 51, loss = 0.30960397\n",
      "Iteration 52, loss = 0.30831186\n",
      "Iteration 53, loss = 0.30875781\n",
      "Iteration 54, loss = 0.30638841\n",
      "Iteration 55, loss = 0.30582517\n",
      "Iteration 56, loss = 0.30586202\n",
      "Iteration 57, loss = 0.30541178\n",
      "Iteration 58, loss = 0.30565259\n",
      "Iteration 59, loss = 0.30724777\n",
      "Iteration 60, loss = 0.30468320\n",
      "Iteration 61, loss = 0.30458815\n",
      "Iteration 62, loss = 0.30429208\n",
      "Iteration 63, loss = 0.30280188\n",
      "Iteration 64, loss = 0.30412640\n",
      "Iteration 65, loss = 0.30409548\n",
      "Iteration 66, loss = 0.30230755\n",
      "Iteration 67, loss = 0.30176478\n",
      "Iteration 68, loss = 0.30086736\n",
      "Iteration 69, loss = 0.30271465\n",
      "Iteration 70, loss = 0.30141103\n",
      "Iteration 71, loss = 0.29973506\n",
      "Iteration 72, loss = 0.30002395\n",
      "Iteration 73, loss = 0.29961199\n",
      "Iteration 74, loss = 0.30062223\n",
      "Iteration 75, loss = 0.29899282\n",
      "Iteration 76, loss = 0.30040224\n",
      "Iteration 77, loss = 0.29826066\n",
      "Iteration 78, loss = 0.29864140\n",
      "Iteration 79, loss = 0.29710031\n",
      "Iteration 80, loss = 0.29694070\n",
      "Iteration 81, loss = 0.29713180\n",
      "Iteration 82, loss = 0.29581512\n",
      "Iteration 83, loss = 0.29500409\n",
      "Iteration 84, loss = 0.29572428\n",
      "Iteration 85, loss = 0.29552512\n",
      "Iteration 86, loss = 0.29601095\n",
      "Iteration 87, loss = 0.29453066\n",
      "Iteration 88, loss = 0.29607123\n",
      "Iteration 89, loss = 0.29322741\n",
      "Iteration 90, loss = 0.29260240\n",
      "Iteration 91, loss = 0.29471358\n",
      "Iteration 92, loss = 0.29369791\n",
      "Iteration 93, loss = 0.29093380\n",
      "Iteration 94, loss = 0.29228399\n",
      "Iteration 95, loss = 0.29410219\n",
      "Iteration 96, loss = 0.29181730\n",
      "Iteration 97, loss = 0.29084720\n",
      "Iteration 98, loss = 0.29111054\n",
      "Iteration 99, loss = 0.29091412\n",
      "Iteration 100, loss = 0.29052060\n",
      "Iteration 101, loss = 0.28984900\n",
      "Iteration 102, loss = 0.29044697\n",
      "Iteration 103, loss = 0.29199668\n",
      "Iteration 104, loss = 0.28862744\n",
      "Iteration 105, loss = 0.28961474\n",
      "Iteration 106, loss = 0.28870757\n",
      "Iteration 107, loss = 0.29076001\n",
      "Iteration 108, loss = 0.28941271\n",
      "Iteration 109, loss = 0.28859805\n",
      "Iteration 110, loss = 0.28747258\n",
      "Iteration 111, loss = 0.28741818\n",
      "Iteration 112, loss = 0.28847380\n",
      "Iteration 113, loss = 0.28642494\n",
      "Iteration 114, loss = 0.28653421\n",
      "Iteration 115, loss = 0.28831385\n",
      "Iteration 116, loss = 0.28755436\n",
      "Iteration 117, loss = 0.28492345\n",
      "Iteration 118, loss = 0.28668902\n",
      "Iteration 119, loss = 0.28516965\n",
      "Iteration 120, loss = 0.28603407\n",
      "Iteration 121, loss = 0.28449249\n",
      "Iteration 122, loss = 0.28605974\n",
      "Iteration 123, loss = 0.28511919\n",
      "Iteration 124, loss = 0.28554563\n",
      "Iteration 125, loss = 0.28685457\n",
      "Iteration 126, loss = 0.28466404\n",
      "Iteration 127, loss = 0.28447647\n",
      "Iteration 128, loss = 0.28349236\n",
      "Iteration 129, loss = 0.28249555\n",
      "Iteration 130, loss = 0.28489159\n",
      "Iteration 131, loss = 0.28499345\n",
      "Iteration 132, loss = 0.28301308\n",
      "Iteration 133, loss = 0.28314526\n",
      "Iteration 134, loss = 0.28459123\n",
      "Iteration 135, loss = 0.28201068\n",
      "Iteration 136, loss = 0.28438925\n",
      "Iteration 137, loss = 0.28271832\n",
      "Iteration 138, loss = 0.28139930\n",
      "Iteration 139, loss = 0.28179985\n",
      "Iteration 140, loss = 0.28112452\n",
      "Iteration 141, loss = 0.28193169\n",
      "Iteration 142, loss = 0.28190013\n",
      "Iteration 143, loss = 0.28364412\n",
      "Iteration 144, loss = 0.28116338\n",
      "Iteration 145, loss = 0.28148028\n",
      "Iteration 146, loss = 0.28099273\n",
      "Iteration 147, loss = 0.28144784\n",
      "Iteration 148, loss = 0.28051203\n",
      "Iteration 149, loss = 0.28021967\n",
      "Iteration 150, loss = 0.28313818\n",
      "Iteration 151, loss = 0.28072138\n",
      "Iteration 152, loss = 0.28494046\n",
      "Iteration 153, loss = 0.27917334\n",
      "Iteration 154, loss = 0.27997192\n",
      "Iteration 155, loss = 0.27906875\n",
      "Iteration 156, loss = 0.28034404\n",
      "Iteration 157, loss = 0.27935840\n",
      "Iteration 158, loss = 0.27905429\n",
      "Iteration 159, loss = 0.27876282\n",
      "Iteration 160, loss = 0.27927669\n",
      "Iteration 161, loss = 0.28001936\n",
      "Iteration 162, loss = 0.27753717\n",
      "Iteration 163, loss = 0.27837144\n",
      "Iteration 164, loss = 0.27948761\n",
      "Iteration 165, loss = 0.27811830\n",
      "Iteration 166, loss = 0.27900624\n",
      "Iteration 167, loss = 0.27910805\n",
      "Iteration 168, loss = 0.27927848\n",
      "Iteration 169, loss = 0.27838193\n",
      "Iteration 170, loss = 0.27751765\n",
      "Iteration 171, loss = 0.27871555\n",
      "Iteration 172, loss = 0.27793446\n",
      "Iteration 173, loss = 0.27694037\n",
      "Iteration 174, loss = 0.27758282\n",
      "Iteration 175, loss = 0.27822887\n",
      "Iteration 176, loss = 0.27672844\n",
      "Iteration 177, loss = 0.27742025\n",
      "Iteration 178, loss = 0.27704876\n",
      "Iteration 179, loss = 0.27593260\n",
      "Iteration 180, loss = 0.27734007\n",
      "Iteration 181, loss = 0.27970999\n",
      "Iteration 182, loss = 0.27789473\n",
      "Iteration 183, loss = 0.27996713\n",
      "Iteration 184, loss = 0.27934139\n",
      "Iteration 185, loss = 0.27689824\n",
      "Iteration 186, loss = 0.27672784\n",
      "Iteration 187, loss = 0.27632584\n",
      "Iteration 188, loss = 0.27560830\n",
      "Iteration 189, loss = 0.27600230\n",
      "Iteration 190, loss = 0.27690637\n",
      "Iteration 191, loss = 0.27749201\n",
      "Iteration 192, loss = 0.27584623\n",
      "Iteration 193, loss = 0.27597178\n",
      "Iteration 194, loss = 0.27629368\n",
      "Iteration 195, loss = 0.27679270\n",
      "Iteration 196, loss = 0.27522457\n",
      "Iteration 197, loss = 0.27433813\n",
      "Iteration 198, loss = 0.27613973\n",
      "Iteration 199, loss = 0.27626612\n",
      "Iteration 200, loss = 0.27573359\n",
      "Iteration 201, loss = 0.27552043\n",
      "Iteration 202, loss = 0.27500918\n",
      "Iteration 203, loss = 0.27576109\n",
      "Iteration 204, loss = 0.27656273\n",
      "Iteration 205, loss = 0.27718754\n",
      "Iteration 206, loss = 0.27443727\n",
      "Iteration 207, loss = 0.27815284\n",
      "Iteration 208, loss = 0.27704394\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Results case  1 :\n",
      "Accurancy:  0.7561306362300825\n",
      "TN: 0.0657701435190417\n",
      "FP: 0.00429427053904396\n",
      "FN: 0.23957509323087353\n",
      "TP: 0.6903604927110408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:916: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.63307274\n",
      "Iteration 2, loss = 0.61012593\n",
      "Iteration 3, loss = 0.60943945\n",
      "Iteration 4, loss = 0.60877215\n",
      "Iteration 5, loss = 0.60775126\n",
      "Iteration 6, loss = 0.60652167\n",
      "Iteration 7, loss = 0.60503632\n",
      "Iteration 8, loss = 0.60338995\n",
      "Iteration 9, loss = 0.60041990\n",
      "Iteration 10, loss = 0.59700489\n",
      "Iteration 11, loss = 0.59169644\n",
      "Iteration 12, loss = 0.58478779\n",
      "Iteration 13, loss = 0.57581081\n",
      "Iteration 14, loss = 0.56527388\n",
      "Iteration 15, loss = 0.55147922\n",
      "Iteration 16, loss = 0.53416551\n",
      "Iteration 17, loss = 0.51091985\n",
      "Iteration 18, loss = 0.48960677\n",
      "Iteration 19, loss = 0.46512966\n",
      "Iteration 20, loss = 0.44434546\n",
      "Iteration 21, loss = 0.42465892\n",
      "Iteration 22, loss = 0.40750209\n",
      "Iteration 23, loss = 0.39357734\n",
      "Iteration 24, loss = 0.38070470\n",
      "Iteration 25, loss = 0.37086617\n",
      "Iteration 26, loss = 0.36350577\n",
      "Iteration 27, loss = 0.35740515\n",
      "Iteration 28, loss = 0.35206851\n",
      "Iteration 29, loss = 0.34434622\n",
      "Iteration 30, loss = 0.34205167\n",
      "Iteration 31, loss = 0.33835330\n",
      "Iteration 32, loss = 0.33508472\n",
      "Iteration 33, loss = 0.33278709\n",
      "Iteration 34, loss = 0.33044079\n",
      "Iteration 35, loss = 0.32829512\n",
      "Iteration 36, loss = 0.32557923\n",
      "Iteration 37, loss = 0.32460868\n",
      "Iteration 38, loss = 0.32307732\n",
      "Iteration 39, loss = 0.32365813\n",
      "Iteration 40, loss = 0.32153551\n",
      "Iteration 41, loss = 0.32029694\n",
      "Iteration 42, loss = 0.31858845\n",
      "Iteration 43, loss = 0.31787726\n",
      "Iteration 44, loss = 0.31767668\n",
      "Iteration 45, loss = 0.31758723\n",
      "Iteration 46, loss = 0.31647046\n",
      "Iteration 47, loss = 0.31518456\n",
      "Iteration 48, loss = 0.31598572\n",
      "Iteration 49, loss = 0.31420301\n",
      "Iteration 50, loss = 0.31846013\n",
      "Iteration 51, loss = 0.31453403\n",
      "Iteration 52, loss = 0.31303885\n",
      "Iteration 53, loss = 0.31337305\n",
      "Iteration 54, loss = 0.31065615\n",
      "Iteration 55, loss = 0.31024011\n",
      "Iteration 56, loss = 0.31189160\n",
      "Iteration 57, loss = 0.30941226\n",
      "Iteration 58, loss = 0.30811515\n",
      "Iteration 59, loss = 0.30841325\n",
      "Iteration 60, loss = 0.30721599\n",
      "Iteration 61, loss = 0.30807247\n",
      "Iteration 62, loss = 0.30666802\n",
      "Iteration 63, loss = 0.30744150\n",
      "Iteration 64, loss = 0.30621429\n",
      "Iteration 65, loss = 0.30663670\n",
      "Iteration 66, loss = 0.30543867\n",
      "Iteration 67, loss = 0.30558025\n",
      "Iteration 68, loss = 0.30562331\n",
      "Iteration 69, loss = 0.30548021\n",
      "Iteration 70, loss = 0.30556164\n",
      "Iteration 71, loss = 0.30331842\n",
      "Iteration 72, loss = 0.30450048\n",
      "Iteration 73, loss = 0.30377845\n",
      "Iteration 74, loss = 0.30301335\n",
      "Iteration 75, loss = 0.30247582\n",
      "Iteration 76, loss = 0.30356440\n",
      "Iteration 77, loss = 0.30245275\n",
      "Iteration 78, loss = 0.30273765\n",
      "Iteration 79, loss = 0.30257568\n",
      "Iteration 80, loss = 0.30061074\n",
      "Iteration 81, loss = 0.30132856\n",
      "Iteration 82, loss = 0.30145856\n",
      "Iteration 83, loss = 0.30083392\n",
      "Iteration 84, loss = 0.29855097\n",
      "Iteration 85, loss = 0.29922404\n",
      "Iteration 86, loss = 0.29971789\n",
      "Iteration 87, loss = 0.29932716\n",
      "Iteration 88, loss = 0.29880719\n",
      "Iteration 89, loss = 0.29703541\n",
      "Iteration 90, loss = 0.29787497\n",
      "Iteration 91, loss = 0.29819176\n",
      "Iteration 92, loss = 0.29692949\n",
      "Iteration 93, loss = 0.29685324\n",
      "Iteration 94, loss = 0.29656359\n",
      "Iteration 95, loss = 0.29801030\n",
      "Iteration 96, loss = 0.29640311\n",
      "Iteration 97, loss = 0.29668088\n",
      "Iteration 98, loss = 0.30214258\n",
      "Iteration 99, loss = 0.29562172\n",
      "Iteration 100, loss = 0.29711680\n",
      "Iteration 101, loss = 0.29455241\n",
      "Iteration 102, loss = 0.29457149\n",
      "Iteration 103, loss = 0.29444457\n",
      "Iteration 104, loss = 0.29445463\n",
      "Iteration 105, loss = 0.29468734\n",
      "Iteration 106, loss = 0.29598978\n",
      "Iteration 107, loss = 0.29371080\n",
      "Iteration 108, loss = 0.29474485\n",
      "Iteration 109, loss = 0.29477495\n",
      "Iteration 110, loss = 0.29524077\n",
      "Iteration 111, loss = 0.29511901\n",
      "Iteration 112, loss = 0.29431619\n",
      "Iteration 113, loss = 0.29287861\n",
      "Iteration 114, loss = 0.29274683\n",
      "Iteration 115, loss = 0.29224400\n",
      "Iteration 116, loss = 0.29327379\n",
      "Iteration 117, loss = 0.29321989\n",
      "Iteration 118, loss = 0.29183600\n",
      "Iteration 119, loss = 0.29203230\n",
      "Iteration 120, loss = 0.29384442\n",
      "Iteration 121, loss = 0.29165123\n",
      "Iteration 122, loss = 0.29404213\n",
      "Iteration 123, loss = 0.29273126\n",
      "Iteration 124, loss = 0.29002153\n",
      "Iteration 125, loss = 0.29160912\n",
      "Iteration 126, loss = 0.29433686\n",
      "Iteration 127, loss = 0.29028590\n",
      "Iteration 128, loss = 0.29222602\n",
      "Iteration 129, loss = 0.29124343\n",
      "Iteration 130, loss = 0.29083040\n",
      "Iteration 131, loss = 0.29210196\n",
      "Iteration 132, loss = 0.29004763\n",
      "Iteration 133, loss = 0.28956987\n",
      "Iteration 134, loss = 0.28984889\n",
      "Iteration 135, loss = 0.28956144\n",
      "Iteration 136, loss = 0.28870734\n",
      "Iteration 137, loss = 0.29113131\n",
      "Iteration 138, loss = 0.28895178\n",
      "Iteration 139, loss = 0.28901386\n",
      "Iteration 140, loss = 0.28919459\n",
      "Iteration 141, loss = 0.28971516\n",
      "Iteration 142, loss = 0.28929251\n",
      "Iteration 143, loss = 0.28876522\n",
      "Iteration 144, loss = 0.28923337\n",
      "Iteration 145, loss = 0.29015770\n",
      "Iteration 146, loss = 0.28768571\n",
      "Iteration 147, loss = 0.29092958\n",
      "Iteration 148, loss = 0.28830999\n",
      "Iteration 149, loss = 0.29197931\n",
      "Iteration 150, loss = 0.28896963\n",
      "Iteration 151, loss = 0.28752952\n",
      "Iteration 152, loss = 0.28744179\n",
      "Iteration 153, loss = 0.28759258\n",
      "Iteration 154, loss = 0.28825540\n",
      "Iteration 155, loss = 0.28664257\n",
      "Iteration 156, loss = 0.28869036\n",
      "Iteration 157, loss = 0.28940887\n",
      "Iteration 158, loss = 0.28769999\n",
      "Iteration 159, loss = 0.28766766\n",
      "Iteration 160, loss = 0.28647248\n",
      "Iteration 161, loss = 0.28788006\n",
      "Iteration 162, loss = 0.28817617\n",
      "Iteration 163, loss = 0.28958006\n",
      "Iteration 164, loss = 0.28803304\n",
      "Iteration 165, loss = 0.28793853\n",
      "Iteration 166, loss = 0.28595250\n",
      "Iteration 167, loss = 0.28605534\n",
      "Iteration 168, loss = 0.28616951\n",
      "Iteration 169, loss = 0.28648595\n",
      "Iteration 170, loss = 0.28553498\n",
      "Iteration 171, loss = 0.28666262\n",
      "Iteration 172, loss = 0.28556748\n",
      "Iteration 173, loss = 0.28507257\n",
      "Iteration 174, loss = 0.28455833\n",
      "Iteration 175, loss = 0.28563882\n",
      "Iteration 176, loss = 0.28671750\n",
      "Iteration 177, loss = 0.28475450\n",
      "Iteration 178, loss = 0.28812210\n",
      "Iteration 179, loss = 0.28679774\n",
      "Iteration 180, loss = 0.28928814\n",
      "Iteration 181, loss = 0.28567814\n",
      "Iteration 182, loss = 0.28635251\n",
      "Iteration 183, loss = 0.28534430\n",
      "Iteration 184, loss = 0.28456195\n",
      "Iteration 185, loss = 0.28335153\n",
      "Iteration 186, loss = 0.28460549\n",
      "Iteration 187, loss = 0.28717530\n",
      "Iteration 188, loss = 0.28394938\n",
      "Iteration 189, loss = 0.28534802\n",
      "Iteration 190, loss = 0.28713864\n",
      "Iteration 191, loss = 0.28443454\n",
      "Iteration 192, loss = 0.28556291\n",
      "Iteration 193, loss = 0.28407107\n",
      "Iteration 194, loss = 0.28350463\n",
      "Iteration 195, loss = 0.28443268\n",
      "Iteration 196, loss = 0.28392602\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Results case  2 :\n",
      "Accurancy:  0.7842694089727653\n",
      "TN: 0.06452706520510793\n",
      "FP: 0.005537348852977738\n",
      "FN: 0.21019324217425697\n",
      "TP: 0.7197423437676573\n",
      "Iteration 1, loss = 0.68279446\n",
      "Iteration 2, loss = 0.61089178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:916: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 0.60934244\n",
      "Iteration 4, loss = 0.60855783\n",
      "Iteration 5, loss = 0.60730071\n",
      "Iteration 6, loss = 0.60659319\n",
      "Iteration 7, loss = 0.60467501\n",
      "Iteration 8, loss = 0.60262254\n",
      "Iteration 9, loss = 0.59963561\n",
      "Iteration 10, loss = 0.59523244\n",
      "Iteration 11, loss = 0.58897317\n",
      "Iteration 12, loss = 0.58061379\n",
      "Iteration 13, loss = 0.56934786\n",
      "Iteration 14, loss = 0.55501441\n",
      "Iteration 15, loss = 0.53657056\n",
      "Iteration 16, loss = 0.51548807\n",
      "Iteration 17, loss = 0.49317053\n",
      "Iteration 18, loss = 0.46905340\n",
      "Iteration 19, loss = 0.44695480\n",
      "Iteration 20, loss = 0.42523455\n",
      "Iteration 21, loss = 0.40952396\n",
      "Iteration 22, loss = 0.39417778\n",
      "Iteration 23, loss = 0.38243340\n",
      "Iteration 24, loss = 0.37294290\n",
      "Iteration 25, loss = 0.36259623\n",
      "Iteration 26, loss = 0.35546051\n",
      "Iteration 27, loss = 0.34952761\n",
      "Iteration 28, loss = 0.34752773\n",
      "Iteration 29, loss = 0.34192757\n",
      "Iteration 30, loss = 0.33773626\n",
      "Iteration 31, loss = 0.33979535\n",
      "Iteration 32, loss = 0.33265194\n",
      "Iteration 33, loss = 0.32976604\n",
      "Iteration 34, loss = 0.32829578\n",
      "Iteration 35, loss = 0.32854294\n",
      "Iteration 36, loss = 0.32578160\n",
      "Iteration 37, loss = 0.32484619\n",
      "Iteration 38, loss = 0.32283590\n",
      "Iteration 39, loss = 0.32345817\n",
      "Iteration 40, loss = 0.32004635\n",
      "Iteration 41, loss = 0.31935906\n",
      "Iteration 42, loss = 0.31959291\n",
      "Iteration 43, loss = 0.31782633\n",
      "Iteration 44, loss = 0.31606136\n",
      "Iteration 45, loss = 0.31518919\n",
      "Iteration 46, loss = 0.31724261\n",
      "Iteration 47, loss = 0.31291821\n",
      "Iteration 48, loss = 0.31232212\n",
      "Iteration 49, loss = 0.31199166\n",
      "Iteration 50, loss = 0.31489992\n",
      "Iteration 51, loss = 0.31026235\n",
      "Iteration 52, loss = 0.30983178\n",
      "Iteration 53, loss = 0.30875552\n",
      "Iteration 54, loss = 0.30859949\n",
      "Iteration 55, loss = 0.30865357\n",
      "Iteration 56, loss = 0.30849064\n",
      "Iteration 57, loss = 0.30740492\n",
      "Iteration 58, loss = 0.30784586\n",
      "Iteration 59, loss = 0.30550471\n",
      "Iteration 60, loss = 0.30611267\n",
      "Iteration 61, loss = 0.30457040\n",
      "Iteration 62, loss = 0.30526788\n",
      "Iteration 63, loss = 0.30319621\n",
      "Iteration 64, loss = 0.30167990\n",
      "Iteration 65, loss = 0.30178728\n",
      "Iteration 66, loss = 0.30311170\n",
      "Iteration 67, loss = 0.30238242\n",
      "Iteration 68, loss = 0.29988847\n",
      "Iteration 69, loss = 0.30004345\n",
      "Iteration 70, loss = 0.29941589\n",
      "Iteration 71, loss = 0.30030871\n",
      "Iteration 72, loss = 0.30013162\n",
      "Iteration 73, loss = 0.29820215\n",
      "Iteration 74, loss = 0.29785389\n",
      "Iteration 75, loss = 0.29725116\n",
      "Iteration 76, loss = 0.29820323\n",
      "Iteration 77, loss = 0.29974872\n",
      "Iteration 78, loss = 0.29564379\n",
      "Iteration 79, loss = 0.29855433\n",
      "Iteration 80, loss = 0.29739674\n",
      "Iteration 81, loss = 0.29581340\n",
      "Iteration 82, loss = 0.29464792\n",
      "Iteration 83, loss = 0.29530904\n",
      "Iteration 84, loss = 0.29396888\n",
      "Iteration 85, loss = 0.29419074\n",
      "Iteration 86, loss = 0.29359101\n",
      "Iteration 87, loss = 0.29253139\n",
      "Iteration 88, loss = 0.29243594\n",
      "Iteration 89, loss = 0.29463091\n",
      "Iteration 90, loss = 0.29197514\n",
      "Iteration 91, loss = 0.29195441\n",
      "Iteration 92, loss = 0.29139567\n",
      "Iteration 93, loss = 0.29200448\n",
      "Iteration 94, loss = 0.29117432\n",
      "Iteration 95, loss = 0.29044645\n",
      "Iteration 96, loss = 0.29122444\n",
      "Iteration 97, loss = 0.28900838\n",
      "Iteration 98, loss = 0.29604212\n",
      "Iteration 99, loss = 0.28939393\n",
      "Iteration 100, loss = 0.28857199\n",
      "Iteration 101, loss = 0.29113445\n",
      "Iteration 102, loss = 0.29000587\n",
      "Iteration 103, loss = 0.28932501\n",
      "Iteration 104, loss = 0.28798173\n",
      "Iteration 105, loss = 0.28713860\n",
      "Iteration 106, loss = 0.29320318\n",
      "Iteration 107, loss = 0.28693138\n",
      "Iteration 108, loss = 0.28654874\n",
      "Iteration 109, loss = 0.28642822\n",
      "Iteration 110, loss = 0.28706317\n",
      "Iteration 111, loss = 0.28737302\n",
      "Iteration 112, loss = 0.28599122\n",
      "Iteration 113, loss = 0.28551011\n",
      "Iteration 114, loss = 0.28611968\n",
      "Iteration 115, loss = 0.28452613\n",
      "Iteration 116, loss = 0.28431325\n",
      "Iteration 117, loss = 0.28446518\n",
      "Iteration 118, loss = 0.28324333\n",
      "Iteration 119, loss = 0.28437781\n",
      "Iteration 120, loss = 0.28546048\n",
      "Iteration 121, loss = 0.28460109\n",
      "Iteration 122, loss = 0.28508651\n",
      "Iteration 123, loss = 0.28283374\n",
      "Iteration 124, loss = 0.28330957\n",
      "Iteration 125, loss = 0.28254833\n",
      "Iteration 126, loss = 0.28380243\n",
      "Iteration 127, loss = 0.28233056\n",
      "Iteration 128, loss = 0.28162699\n",
      "Iteration 129, loss = 0.28279426\n",
      "Iteration 130, loss = 0.28089916\n",
      "Iteration 131, loss = 0.28268926\n",
      "Iteration 132, loss = 0.28123504\n",
      "Iteration 133, loss = 0.28336714\n",
      "Iteration 134, loss = 0.28209468\n",
      "Iteration 135, loss = 0.28147781\n",
      "Iteration 136, loss = 0.28071074\n",
      "Iteration 137, loss = 0.28049175\n",
      "Iteration 138, loss = 0.28066806\n",
      "Iteration 139, loss = 0.27984027\n",
      "Iteration 140, loss = 0.27969970\n",
      "Iteration 141, loss = 0.28015712\n",
      "Iteration 142, loss = 0.27913661\n",
      "Iteration 143, loss = 0.28211986\n",
      "Iteration 144, loss = 0.27876896\n",
      "Iteration 145, loss = 0.27968493\n",
      "Iteration 146, loss = 0.27886863\n",
      "Iteration 147, loss = 0.27912634\n",
      "Iteration 148, loss = 0.28115853\n",
      "Iteration 149, loss = 0.28588083\n",
      "Iteration 150, loss = 0.27836932\n",
      "Iteration 151, loss = 0.27870417\n",
      "Iteration 152, loss = 0.28106693\n",
      "Iteration 153, loss = 0.27825503\n",
      "Iteration 154, loss = 0.27843741\n",
      "Iteration 155, loss = 0.27734614\n",
      "Iteration 156, loss = 0.27790139\n",
      "Iteration 157, loss = 0.27915226\n",
      "Iteration 158, loss = 0.27796018\n",
      "Iteration 159, loss = 0.28006825\n",
      "Iteration 160, loss = 0.27699031\n",
      "Iteration 161, loss = 0.27667786\n",
      "Iteration 162, loss = 0.27675788\n",
      "Iteration 163, loss = 0.27650198\n",
      "Iteration 164, loss = 0.27670008\n",
      "Iteration 165, loss = 0.27701955\n",
      "Iteration 166, loss = 0.27774567\n",
      "Iteration 167, loss = 0.27678774\n",
      "Iteration 168, loss = 0.27721480\n",
      "Iteration 169, loss = 0.27754758\n",
      "Iteration 170, loss = 0.27735236\n",
      "Iteration 171, loss = 0.27700371\n",
      "Iteration 172, loss = 0.27645557\n",
      "Iteration 173, loss = 0.27883285\n",
      "Iteration 174, loss = 0.27503809\n",
      "Iteration 175, loss = 0.27688547\n",
      "Iteration 176, loss = 0.27632834\n",
      "Iteration 177, loss = 0.27515660\n",
      "Iteration 178, loss = 0.27511348\n",
      "Iteration 179, loss = 0.27511142\n",
      "Iteration 180, loss = 0.27740015\n",
      "Iteration 181, loss = 0.27536104\n",
      "Iteration 182, loss = 0.27953871\n",
      "Iteration 183, loss = 0.27711342\n",
      "Iteration 184, loss = 0.27468447\n",
      "Iteration 185, loss = 0.27966684\n",
      "Iteration 186, loss = 0.27587153\n",
      "Iteration 187, loss = 0.27336220\n",
      "Iteration 188, loss = 0.27586750\n",
      "Iteration 189, loss = 0.27467711\n",
      "Iteration 190, loss = 0.27461565\n",
      "Iteration 191, loss = 0.27486795\n",
      "Iteration 192, loss = 0.27628674\n",
      "Iteration 193, loss = 0.27500587\n",
      "Iteration 194, loss = 0.27387571\n",
      "Iteration 195, loss = 0.27306208\n",
      "Iteration 196, loss = 0.27466099\n",
      "Iteration 197, loss = 0.27313968\n",
      "Iteration 198, loss = 0.27699459\n",
      "Iteration 199, loss = 0.27356297\n",
      "Iteration 200, loss = 0.27437978\n",
      "Iteration 201, loss = 0.27346309\n",
      "Iteration 202, loss = 0.27724310\n",
      "Iteration 203, loss = 0.27213598\n",
      "Iteration 204, loss = 0.27684400\n",
      "Iteration 205, loss = 0.27327899\n",
      "Iteration 206, loss = 0.27515111\n",
      "Iteration 207, loss = 0.27491434\n",
      "Iteration 208, loss = 0.27193433\n",
      "Iteration 209, loss = 0.27266369\n",
      "Iteration 210, loss = 0.27372390\n",
      "Iteration 211, loss = 0.27481978\n",
      "Iteration 212, loss = 0.27323867\n",
      "Iteration 213, loss = 0.27166584\n",
      "Iteration 214, loss = 0.27697468\n",
      "Iteration 215, loss = 0.27720863\n",
      "Iteration 216, loss = 0.27365274\n",
      "Iteration 217, loss = 0.27166052\n",
      "Iteration 218, loss = 0.27222096\n",
      "Iteration 219, loss = 0.27190787\n",
      "Iteration 220, loss = 0.27205380\n",
      "Iteration 221, loss = 0.27181394\n",
      "Iteration 222, loss = 0.27265559\n",
      "Iteration 223, loss = 0.27196976\n",
      "Iteration 224, loss = 0.27875534\n",
      "Iteration 225, loss = 0.27096451\n",
      "Iteration 226, loss = 0.27158087\n",
      "Iteration 227, loss = 0.27118207\n",
      "Iteration 228, loss = 0.27502796\n",
      "Iteration 229, loss = 0.27472803\n",
      "Iteration 230, loss = 0.27110345\n",
      "Iteration 231, loss = 0.27054971\n",
      "Iteration 232, loss = 0.27074527\n",
      "Iteration 233, loss = 0.27076966\n",
      "Iteration 234, loss = 0.27144888\n",
      "Iteration 235, loss = 0.27133053\n",
      "Iteration 236, loss = 0.27072828\n",
      "Iteration 237, loss = 0.27061280\n",
      "Iteration 238, loss = 0.27037253\n",
      "Iteration 239, loss = 0.27034042\n",
      "Iteration 240, loss = 0.26983387\n",
      "Iteration 241, loss = 0.27191088\n",
      "Iteration 242, loss = 0.27027071\n",
      "Iteration 243, loss = 0.27309217\n",
      "Iteration 244, loss = 0.27125102\n",
      "Iteration 245, loss = 0.27118787\n",
      "Iteration 246, loss = 0.27026291\n",
      "Iteration 247, loss = 0.27066146\n",
      "Iteration 248, loss = 0.27062042\n",
      "Iteration 249, loss = 0.27286848\n",
      "Iteration 250, loss = 0.27514211\n",
      "Iteration 251, loss = 0.27093545\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results case  3 :\n",
      "Accurancy:  0.7990733416205221\n",
      "TN: 0.06497909368290203\n",
      "FP: 0.005085320375183637\n",
      "FN: 0.19584133800429426\n",
      "TP: 0.73409424793762\n",
      "Iteration 1, loss = 0.61325272\n",
      "Iteration 2, loss = 0.61007743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:916: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 0.60957067\n",
      "Iteration 4, loss = 0.60873549\n",
      "Iteration 5, loss = 0.60716938\n",
      "Iteration 6, loss = 0.60483668\n",
      "Iteration 7, loss = 0.60112633\n",
      "Iteration 8, loss = 0.59537995\n",
      "Iteration 9, loss = 0.58491816\n",
      "Iteration 10, loss = 0.57025057\n",
      "Iteration 11, loss = 0.55020483\n",
      "Iteration 12, loss = 0.52384129\n",
      "Iteration 13, loss = 0.49621742\n",
      "Iteration 14, loss = 0.47015732\n",
      "Iteration 15, loss = 0.44480045\n",
      "Iteration 16, loss = 0.42522200\n",
      "Iteration 17, loss = 0.40661253\n",
      "Iteration 18, loss = 0.39390214\n",
      "Iteration 19, loss = 0.38219235\n",
      "Iteration 20, loss = 0.37271904\n",
      "Iteration 21, loss = 0.36419200\n",
      "Iteration 22, loss = 0.35732923\n",
      "Iteration 23, loss = 0.35499785\n",
      "Iteration 24, loss = 0.35451315\n",
      "Iteration 25, loss = 0.34544571\n",
      "Iteration 26, loss = 0.34261850\n",
      "Iteration 27, loss = 0.34029923\n",
      "Iteration 28, loss = 0.33703660\n",
      "Iteration 29, loss = 0.33591793\n",
      "Iteration 30, loss = 0.33404488\n",
      "Iteration 31, loss = 0.33230750\n",
      "Iteration 32, loss = 0.33154801\n",
      "Iteration 33, loss = 0.33013222\n",
      "Iteration 34, loss = 0.32828266\n",
      "Iteration 35, loss = 0.32977121\n",
      "Iteration 36, loss = 0.32924027\n",
      "Iteration 37, loss = 0.32462175\n",
      "Iteration 38, loss = 0.32339190\n",
      "Iteration 39, loss = 0.32815056\n",
      "Iteration 40, loss = 0.32244065\n",
      "Iteration 41, loss = 0.32040585\n",
      "Iteration 42, loss = 0.32086703\n",
      "Iteration 43, loss = 0.31940026\n",
      "Iteration 44, loss = 0.32029454\n",
      "Iteration 45, loss = 0.31770317\n",
      "Iteration 46, loss = 0.31712169\n",
      "Iteration 47, loss = 0.31624873\n",
      "Iteration 48, loss = 0.31846851\n",
      "Iteration 49, loss = 0.31409124\n",
      "Iteration 50, loss = 0.31355182\n",
      "Iteration 51, loss = 0.31568123\n",
      "Iteration 52, loss = 0.31406776\n",
      "Iteration 53, loss = 0.31231346\n",
      "Iteration 54, loss = 0.31032222\n",
      "Iteration 55, loss = 0.31025547\n",
      "Iteration 56, loss = 0.30978490\n",
      "Iteration 57, loss = 0.31018864\n",
      "Iteration 58, loss = 0.31040660\n",
      "Iteration 59, loss = 0.30758204\n",
      "Iteration 60, loss = 0.30678667\n",
      "Iteration 61, loss = 0.30789093\n",
      "Iteration 62, loss = 0.30715150\n",
      "Iteration 63, loss = 0.30700119\n",
      "Iteration 64, loss = 0.30576215\n",
      "Iteration 65, loss = 0.30656908\n",
      "Iteration 66, loss = 0.30496653\n",
      "Iteration 67, loss = 0.30436185\n",
      "Iteration 68, loss = 0.30420901\n",
      "Iteration 69, loss = 0.30281216\n",
      "Iteration 70, loss = 0.30198182\n",
      "Iteration 71, loss = 0.30162273\n",
      "Iteration 72, loss = 0.30329854\n",
      "Iteration 73, loss = 0.30119187\n",
      "Iteration 74, loss = 0.30079566\n",
      "Iteration 75, loss = 0.29986128\n",
      "Iteration 76, loss = 0.30099295\n",
      "Iteration 77, loss = 0.30341943\n",
      "Iteration 78, loss = 0.29937384\n",
      "Iteration 79, loss = 0.29844663\n",
      "Iteration 80, loss = 0.30085912\n",
      "Iteration 81, loss = 0.29873310\n",
      "Iteration 82, loss = 0.29710405\n",
      "Iteration 83, loss = 0.29688981\n",
      "Iteration 84, loss = 0.29604020\n",
      "Iteration 85, loss = 0.29542908\n",
      "Iteration 86, loss = 0.29607906\n",
      "Iteration 87, loss = 0.29648822\n",
      "Iteration 88, loss = 0.29423558\n",
      "Iteration 89, loss = 0.29507091\n",
      "Iteration 90, loss = 0.29631665\n",
      "Iteration 91, loss = 0.29546700\n",
      "Iteration 92, loss = 0.29340252\n",
      "Iteration 93, loss = 0.29348928\n",
      "Iteration 94, loss = 0.29268205\n",
      "Iteration 95, loss = 0.29296814\n",
      "Iteration 96, loss = 0.29206391\n",
      "Iteration 97, loss = 0.29097628\n",
      "Iteration 98, loss = 0.29073366\n",
      "Iteration 99, loss = 0.29090497\n",
      "Iteration 100, loss = 0.29191127\n",
      "Iteration 101, loss = 0.29163465\n",
      "Iteration 102, loss = 0.29046883\n",
      "Iteration 103, loss = 0.29126860\n",
      "Iteration 104, loss = 0.29227692\n",
      "Iteration 105, loss = 0.29023501\n",
      "Iteration 106, loss = 0.28921315\n",
      "Iteration 107, loss = 0.28830689\n",
      "Iteration 108, loss = 0.29012737\n",
      "Iteration 109, loss = 0.28954883\n",
      "Iteration 110, loss = 0.28892792\n",
      "Iteration 111, loss = 0.28829718\n",
      "Iteration 112, loss = 0.28787555\n",
      "Iteration 113, loss = 0.28727587\n",
      "Iteration 114, loss = 0.28872998\n",
      "Iteration 115, loss = 0.28916914\n",
      "Iteration 116, loss = 0.28739527\n",
      "Iteration 117, loss = 0.28741209\n",
      "Iteration 118, loss = 0.28724354\n",
      "Iteration 119, loss = 0.28565962\n",
      "Iteration 120, loss = 0.28744576\n",
      "Iteration 121, loss = 0.28664023\n",
      "Iteration 122, loss = 0.28889071\n",
      "Iteration 123, loss = 0.28459377\n",
      "Iteration 124, loss = 0.28856908\n",
      "Iteration 125, loss = 0.28485803\n",
      "Iteration 126, loss = 0.28744834\n",
      "Iteration 127, loss = 0.28541098\n",
      "Iteration 128, loss = 0.28573037\n",
      "Iteration 129, loss = 0.28612032\n",
      "Iteration 130, loss = 0.28424383\n",
      "Iteration 131, loss = 0.28531847\n",
      "Iteration 132, loss = 0.28318634\n",
      "Iteration 133, loss = 0.28404172\n",
      "Iteration 134, loss = 0.28284894\n",
      "Iteration 135, loss = 0.28339818\n",
      "Iteration 136, loss = 0.28181497\n",
      "Iteration 137, loss = 0.28263127\n",
      "Iteration 138, loss = 0.28709220\n",
      "Iteration 139, loss = 0.28216433\n",
      "Iteration 140, loss = 0.28243039\n",
      "Iteration 141, loss = 0.28377280\n",
      "Iteration 142, loss = 0.28197523\n",
      "Iteration 143, loss = 0.28432582\n",
      "Iteration 144, loss = 0.28284309\n",
      "Iteration 145, loss = 0.28224185\n",
      "Iteration 146, loss = 0.28411668\n",
      "Iteration 147, loss = 0.28236556\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Results case  4 :\n",
      "Accurancy:  0.8215617583907786\n",
      "TN: 0.06384902248841677\n",
      "FP: 0.006215391569668889\n",
      "FN: 0.17222285003955248\n",
      "TP: 0.7577127359023619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:916: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.65500667\n",
      "Iteration 2, loss = 0.61117248\n",
      "Iteration 3, loss = 0.60956288\n",
      "Iteration 4, loss = 0.60897130\n",
      "Iteration 5, loss = 0.60815432\n",
      "Iteration 6, loss = 0.60710507\n",
      "Iteration 7, loss = 0.60610381\n",
      "Iteration 8, loss = 0.60461130\n",
      "Iteration 9, loss = 0.60284890\n",
      "Iteration 10, loss = 0.59988610\n",
      "Iteration 11, loss = 0.59580250\n",
      "Iteration 12, loss = 0.59103646\n",
      "Iteration 13, loss = 0.58416388\n",
      "Iteration 14, loss = 0.57531591\n",
      "Iteration 15, loss = 0.56445419\n",
      "Iteration 16, loss = 0.55097818\n",
      "Iteration 17, loss = 0.53400820\n",
      "Iteration 18, loss = 0.51378065\n",
      "Iteration 19, loss = 0.49329516\n",
      "Iteration 20, loss = 0.47091598\n",
      "Iteration 21, loss = 0.45061327\n",
      "Iteration 22, loss = 0.43170257\n",
      "Iteration 23, loss = 0.41484023\n",
      "Iteration 24, loss = 0.40166899\n",
      "Iteration 25, loss = 0.38776345\n",
      "Iteration 26, loss = 0.37819949\n",
      "Iteration 27, loss = 0.36999157\n",
      "Iteration 28, loss = 0.36243684\n",
      "Iteration 29, loss = 0.35592086\n",
      "Iteration 30, loss = 0.35015224\n",
      "Iteration 31, loss = 0.34507727\n",
      "Iteration 32, loss = 0.34218542\n",
      "Iteration 33, loss = 0.33914920\n",
      "Iteration 34, loss = 0.33556754\n",
      "Iteration 35, loss = 0.33372806\n",
      "Iteration 36, loss = 0.33247098\n",
      "Iteration 37, loss = 0.32973955\n",
      "Iteration 38, loss = 0.32832095\n",
      "Iteration 39, loss = 0.32571297\n",
      "Iteration 40, loss = 0.32610346\n",
      "Iteration 41, loss = 0.32656738\n",
      "Iteration 42, loss = 0.32272707\n",
      "Iteration 43, loss = 0.32317307\n",
      "Iteration 44, loss = 0.32133311\n",
      "Iteration 45, loss = 0.31953456\n",
      "Iteration 46, loss = 0.31797329\n",
      "Iteration 47, loss = 0.31877240\n",
      "Iteration 48, loss = 0.31702920\n",
      "Iteration 49, loss = 0.31685648\n",
      "Iteration 50, loss = 0.31707905\n",
      "Iteration 51, loss = 0.31664524\n",
      "Iteration 52, loss = 0.31470025\n",
      "Iteration 53, loss = 0.31417590\n",
      "Iteration 54, loss = 0.31316226\n",
      "Iteration 55, loss = 0.31385677\n",
      "Iteration 56, loss = 0.31286842\n",
      "Iteration 57, loss = 0.31244843\n",
      "Iteration 58, loss = 0.31159108\n",
      "Iteration 59, loss = 0.31180301\n",
      "Iteration 60, loss = 0.31044534\n",
      "Iteration 61, loss = 0.30914202\n",
      "Iteration 62, loss = 0.30878289\n",
      "Iteration 63, loss = 0.31167326\n",
      "Iteration 64, loss = 0.30833236\n",
      "Iteration 65, loss = 0.30746564\n",
      "Iteration 66, loss = 0.30670422\n",
      "Iteration 67, loss = 0.30836481\n",
      "Iteration 68, loss = 0.30642592\n",
      "Iteration 69, loss = 0.30739649\n",
      "Iteration 70, loss = 0.30612049\n",
      "Iteration 71, loss = 0.30608687\n",
      "Iteration 72, loss = 0.30504685\n",
      "Iteration 73, loss = 0.30361768\n",
      "Iteration 74, loss = 0.30540431\n",
      "Iteration 75, loss = 0.30474204\n",
      "Iteration 76, loss = 0.30335241\n",
      "Iteration 77, loss = 0.30462973\n",
      "Iteration 78, loss = 0.30344840\n",
      "Iteration 79, loss = 0.30342732\n",
      "Iteration 80, loss = 0.30224295\n",
      "Iteration 81, loss = 0.30256172\n",
      "Iteration 82, loss = 0.30403085\n",
      "Iteration 83, loss = 0.30214464\n",
      "Iteration 84, loss = 0.30159650\n",
      "Iteration 85, loss = 0.30189112\n",
      "Iteration 86, loss = 0.30098816\n",
      "Iteration 87, loss = 0.29989363\n",
      "Iteration 88, loss = 0.30026152\n",
      "Iteration 89, loss = 0.30011064\n",
      "Iteration 90, loss = 0.30051079\n",
      "Iteration 91, loss = 0.30035826\n",
      "Iteration 92, loss = 0.30108079\n",
      "Iteration 93, loss = 0.30028383\n",
      "Iteration 94, loss = 0.30212360\n",
      "Iteration 95, loss = 0.30161723\n",
      "Iteration 96, loss = 0.29767607\n",
      "Iteration 97, loss = 0.29971781\n",
      "Iteration 98, loss = 0.29835136\n",
      "Iteration 99, loss = 0.29748220\n",
      "Iteration 100, loss = 0.29740049\n",
      "Iteration 101, loss = 0.29791868\n",
      "Iteration 102, loss = 0.29689751\n",
      "Iteration 103, loss = 0.29636877\n",
      "Iteration 104, loss = 0.29702414\n",
      "Iteration 105, loss = 0.29537576\n",
      "Iteration 106, loss = 0.29564596\n",
      "Iteration 107, loss = 0.29503375\n",
      "Iteration 108, loss = 0.29510512\n",
      "Iteration 109, loss = 0.29522208\n",
      "Iteration 110, loss = 0.29843789\n",
      "Iteration 111, loss = 0.29488486\n",
      "Iteration 112, loss = 0.29519462\n",
      "Iteration 113, loss = 0.29408833\n",
      "Iteration 114, loss = 0.29416759\n",
      "Iteration 115, loss = 0.29445788\n",
      "Iteration 116, loss = 0.29422690\n",
      "Iteration 117, loss = 0.29250240\n",
      "Iteration 118, loss = 0.29275549\n",
      "Iteration 119, loss = 0.29278067\n",
      "Iteration 120, loss = 0.29154599\n",
      "Iteration 121, loss = 0.29206535\n",
      "Iteration 122, loss = 0.29367235\n",
      "Iteration 123, loss = 0.29200304\n",
      "Iteration 124, loss = 0.29131803\n",
      "Iteration 125, loss = 0.29104891\n",
      "Iteration 126, loss = 0.29379119\n",
      "Iteration 127, loss = 0.29667704\n",
      "Iteration 128, loss = 0.29084552\n",
      "Iteration 129, loss = 0.29185500\n",
      "Iteration 130, loss = 0.29060113\n",
      "Iteration 131, loss = 0.28982178\n",
      "Iteration 132, loss = 0.28978152\n",
      "Iteration 133, loss = 0.28987584\n",
      "Iteration 134, loss = 0.28908192\n",
      "Iteration 135, loss = 0.28863852\n",
      "Iteration 136, loss = 0.28964033\n",
      "Iteration 137, loss = 0.28915104\n",
      "Iteration 138, loss = 0.28794500\n",
      "Iteration 139, loss = 0.28942727\n",
      "Iteration 140, loss = 0.28738735\n",
      "Iteration 141, loss = 0.28797675\n",
      "Iteration 142, loss = 0.28903517\n",
      "Iteration 143, loss = 0.28669948\n",
      "Iteration 144, loss = 0.29090074\n",
      "Iteration 145, loss = 0.28712991\n",
      "Iteration 146, loss = 0.28771735\n",
      "Iteration 147, loss = 0.28674478\n",
      "Iteration 148, loss = 0.28632012\n",
      "Iteration 149, loss = 0.28547540\n",
      "Iteration 150, loss = 0.28588913\n",
      "Iteration 151, loss = 0.28824294\n",
      "Iteration 152, loss = 0.28796405\n",
      "Iteration 153, loss = 0.28690216\n",
      "Iteration 154, loss = 0.28456549\n",
      "Iteration 155, loss = 0.28509422\n",
      "Iteration 156, loss = 0.28447604\n",
      "Iteration 157, loss = 0.28550608\n",
      "Iteration 158, loss = 0.28597316\n",
      "Iteration 159, loss = 0.28434339\n",
      "Iteration 160, loss = 0.28570119\n",
      "Iteration 161, loss = 0.28398623\n",
      "Iteration 162, loss = 0.28330125\n",
      "Iteration 163, loss = 0.28353730\n",
      "Iteration 164, loss = 0.28478932\n",
      "Iteration 165, loss = 0.28359607\n",
      "Iteration 166, loss = 0.28274462\n",
      "Iteration 167, loss = 0.28234775\n",
      "Iteration 168, loss = 0.28318023\n",
      "Iteration 169, loss = 0.28327729\n",
      "Iteration 170, loss = 0.28397390\n",
      "Iteration 171, loss = 0.28393951\n",
      "Iteration 172, loss = 0.28531852\n",
      "Iteration 173, loss = 0.28222563\n",
      "Iteration 174, loss = 0.28093918\n",
      "Iteration 175, loss = 0.28092055\n",
      "Iteration 176, loss = 0.28231640\n",
      "Iteration 177, loss = 0.28241974\n",
      "Iteration 178, loss = 0.28065763\n",
      "Iteration 179, loss = 0.28196729\n",
      "Iteration 180, loss = 0.28084391\n",
      "Iteration 181, loss = 0.28076238\n",
      "Iteration 182, loss = 0.28100019\n",
      "Iteration 183, loss = 0.28033812\n",
      "Iteration 184, loss = 0.28064232\n",
      "Iteration 185, loss = 0.28159797\n",
      "Iteration 186, loss = 0.28176339\n",
      "Iteration 187, loss = 0.27949384\n",
      "Iteration 188, loss = 0.28218455\n",
      "Iteration 189, loss = 0.27892243\n",
      "Iteration 190, loss = 0.27895811\n",
      "Iteration 191, loss = 0.28060251\n",
      "Iteration 192, loss = 0.28441336\n",
      "Iteration 193, loss = 0.28010222\n",
      "Iteration 194, loss = 0.27880269\n",
      "Iteration 195, loss = 0.27916912\n",
      "Iteration 196, loss = 0.28018714\n",
      "Iteration 197, loss = 0.27829698\n",
      "Iteration 198, loss = 0.27984522\n",
      "Iteration 199, loss = 0.27843428\n",
      "Iteration 200, loss = 0.27831724\n",
      "Iteration 201, loss = 0.28129822\n",
      "Iteration 202, loss = 0.27855409\n",
      "Iteration 203, loss = 0.27903378\n",
      "Iteration 204, loss = 0.28020931\n",
      "Iteration 205, loss = 0.28113673\n",
      "Iteration 206, loss = 0.27999451\n",
      "Iteration 207, loss = 0.28174542\n",
      "Iteration 208, loss = 0.27853145\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Results case  5 :\n",
      "Accurancy:  0.8074358684597129\n",
      "TN: 0.06407503672731382\n",
      "FP: 0.0059893773307718385\n",
      "FN: 0.1865747542095152\n",
      "TP: 0.7433608317323992\n"
     ]
    }
   ],
   "source": [
    "# mlp train test\n",
    "\n",
    "# set number of test and initializate variables\n",
    "number_test = 6\n",
    "avg_accurancy, avg_tn, avg_fp, avg_fn, avg_tp = (0,0,0,0,0)\n",
    "\n",
    "# tests\n",
    "for i in range(number_test):\n",
    "    # initializate mlp\n",
    "    clf = MLPClassifier(hidden_layer_sizes=(100,30), max_iter=5000, alpha=0.0001,solver='adam',verbose=10,tol=0.000000001)\n",
    "   \n",
    "    # train mlp\n",
    "    clf.fit(X_train, Y_train)\n",
    "    \n",
    "    # test mlp \n",
    "    Y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # calculate and print results \n",
    "    accurancy = accuracy_score(Y_test, Y_pred)\n",
    "    avg_accurancy = avg_accurancy + accurancy\n",
    "    tn, fp, fn, tp = confusion_matrix(Y_test, Y_pred).ravel()\n",
    "    print(\"Results case \",i,\":\")\n",
    "    print(\"Accurancy: \", accurancy)\n",
    "    tn=tn/(len(Y_pred))\n",
    "    avg_tn = avg_tn + tn\n",
    "    print(\"TN: \" + str(tn))\n",
    "    fp=fp/len(Y_pred) \n",
    "    avg_fp = avg_fp + fp\n",
    "    print(\"FP: \" + str(fp))\n",
    "    fn=fn/len(Y_pred)\n",
    "    avg_fn = avg_fn + fn\n",
    "    print(\"FN: \" + str(fn))\n",
    "    tp=tp/len(Y_pred)\n",
    "    avg_tp = avg_tp + tp\n",
    "    print(\"TP: \" + str(tp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: \n",
      "Accurancy:  0.7900892756243643\n",
      "TN: 0.06467774136437261\n",
      "FP: 0.005386672693713037\n",
      "FN: 0.2045240516819226\n",
      "TP: 0.7254115342599917\n"
     ]
    }
   ],
   "source": [
    "# average testing results\n",
    "print(\"Results: \")\n",
    "print(\"Accurancy: \", avg_accurancy/number_test)\n",
    "print(\"TN: \" + str(avg_tn/number_test))\n",
    "print(\"FP: \" + str(avg_fp/number_test))\n",
    "print(\"FN: \" + str(avg_fn/number_test))\n",
    "print(\"TP: \" + str(avg_tp/number_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
